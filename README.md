ğ—»ğ—¼ğ—±ğ—²-ğ—®ğ—½ğ—½-ğ—°ğ—¶-ğ—°ğ—±
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—˜ğ—»ğ—± ğ——ğ—²ğ˜ƒğ—¦ğ—²ğ—°ğ—¢ğ—½ğ˜€ ğ—–ğ—œ/ğ—–ğ—— ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—”ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ——ğ—¼ğ—°ğ—¸ğ—²ğ—¿, ğ—”ğ—ªğ—¦ ğ—˜ğ—ğ—¦, ğ—§ğ—²ğ—¿ğ—¿ğ—®ğ—³ğ—¼ğ—¿ğ—º, ğ—¦ğ—¼ğ—»ğ—®ğ—¿ğ—¤ğ˜‚ğ—¯ğ—², ğ—¦ğ—»ğ˜†ğ—¸, ğ—£ğ—¿ğ—¼ğ—ºğ—²ğ˜ğ—µğ—²ğ˜‚ğ˜€, ğ—®ğ—»ğ—± ğ—šğ—¿ğ—®ğ—³ğ—®ğ—»ğ—®.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—¢ğ˜ƒğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„

This GitHub Actions workflow automates the build, security scanning, containerization, and deployment of a Node.js application.

The application is packaged as a Docker image, securely pushed to Amazon ECR, and deployed to a Kubernetes (EKS) cluster using predefined manifests.

Before deployment, the pipeline integrates Snyk security scanning to detect vulnerabilities in both application dependencies and the container image, ensuring that only secure artifacts are released.

The deployment phase also includes installation of Prometheus and Grafana monitoring using Helm to provide observability and operational visibility into the Kubernetes cluster.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—¦ğ˜ğ—®ğ˜ğ˜‚ğ˜€

This project is complete and archived.
All cloud resources were destroyed after validation to prevent ongoing costs.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²

GitHub â†’ GitHub Actions â†’ SonarQube â†’ Docker â†’ Snyk â†’ AWS ECR â†’ Kubernetes (EKS) â†’ Monitoring

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—¦ğ—²ğ—°ğ˜‚ğ—¿ğ—¶ğ˜ğ˜† & ğ—¤ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—šğ—®ğ˜ğ—²ğ˜€

â€¢ SonarQube Quality Gate enforcement within GitHub Actions
â€¢ Snyk container vulnerability blocking (HIGH and CRITICAL)
â€¢ Continuous container vulnerability monitoring with Snyk

Note: SonarQube Quality Gates are enforced in the GitHub Actions workflow. A screenshot was not captured during the final successful run, but the Quality Gate stage is implemented and blocks the workflow on failure.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—ğ˜‚ğ—¯ğ—²ğ—¿ğ—»ğ—²ğ˜ğ—²ğ˜€ ğ—–ğ—¹ğ˜‚ğ—¦ğ˜ğ—²ğ—¿ ğ— ğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ (ğ—šğ—¿ğ—®ğ—³ğ—®ğ—»ğ—®)

Grafana dashboards were successfully deployed using the kube-prometheus-stack Helm chart.

On the AWS Free Tier single-node EKS cluster, Prometheus could not be scheduled due to VPC CNI pod density limits, resulting in dashboards displaying â€œNo dataâ€.

This behavior is expected on resource-constrained clusters. In a production-grade setup, this would be resolved by scaling node capacity or optimizing monitoring component resource requests.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš™ï¸ ğ—§ğ—²ğ—°ğ—µ ğ—¦ğ˜ğ—®ğ—°ğ—¸

â€¢ GitHub Actions
â€¢ Docker
â€¢ Terraform
â€¢ AWS (EKS, ECR, VPC)
â€¢ Kubernetes
â€¢ SonarQube
â€¢ Snyk
â€¢ Prometheus & Grafana

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—–ğ—¼ğ˜€ğ˜ ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜

All infrastructure was destroyed after testing using:

terraform destroy
Helm cleanup for monitoring components

This ensures AWS Free Tier cost safety.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğ—¥ğ—²ğ—¹ğ—²ğ—®ğ˜€ğ—²

v1.0-capstone
