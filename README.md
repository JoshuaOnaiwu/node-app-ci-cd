node-app-ci-cd

End-to-End DevSecOps CI/CD Pipeline using GitHub Actions, Docker, AWS EKS, Kubernetes, Snyk, Prometheus, and Grafana.

ğŸ“Œ Project Overview

This project implements a full DevSecOps CI/CD pipeline for a Node.js application deployed to AWS EKS.

The GitHub Actions pipeline automates the build, security validation, containerization, and deployment workflow.

The application is packaged as a Docker image, securely pushed to Amazon ECR, and deployed to a Kubernetes (EKS) cluster using predefined manifests.

Before deployment, the pipeline incorporates Snyk dependency scanning to identify vulnerabilities in the application dependencies, ensuring that only secure artifacts are released.

The deployment process includes automated rollout validation and horizontal pod autoscaling. Monitoring is enabled through Prometheus and Grafana installed via Helm to provide observability into cluster health and application performance.

ğŸ“ Project Status

This project is complete.

All components were successfully:

â€¢ Built
â€¢ Secured
â€¢ Deployed
â€¢ Scaled
â€¢ Monitored
â€¢ Debugged (including CrashLoopBackOff and rollout timeout scenarios)

The infrastructure remains active for demonstration purposes.
In a production environment, infrastructure teardown would be handled via Infrastructure-as-Code to prevent ongoing costs.

ğŸ§± Architecture

GitHub â†’ GitHub Actions â†’ Snyk â†’ Docker â†’ AWS ECR â†’ Kubernetes (EKS) â†’ HPA â†’ Prometheus â†’ Grafana

This reflects a production-style delivery chain:

Application â†’ Container â†’ Registry â†’ Cluster â†’ Autoscaling â†’ Observability â†’ Security

ğŸ” Security & Quality Controls

Snyk dependency scanning is integrated directly into the CI pipeline.

Security enforcement includes:

â€¢ Blocking HIGH severity vulnerabilities
â€¢ Preventing insecure builds from reaching ECR
â€¢ Automated validation before deployment

Container-level scanning was evaluated during implementation, with plan limitations documented.

Health probes and rollout validation ensure only stable workloads are promoted.

ğŸ“Š Kubernetes Cluster Monitoring (Grafana)

Grafana dashboards were successfully deployed using the kube-prometheus-stack Helm chart.

Monitoring components include:

â€¢ Prometheus
â€¢ Grafana
â€¢ kube-state-metrics
â€¢ node-exporter

Dashboards provide visibility into:

â€¢ Pod CPU usage
â€¢ Memory utilization
â€¢ Node health
â€¢ Horizontal Pod Autoscaler behavior

Monitoring was validated against live cluster metrics.

âš™ï¸ CI/CD Pipeline Flow

Continuous Integration:

Checkout repository

Install dependencies

Run tests

Execute Snyk dependency scan

Build Docker image

Tag image using commit SHA

Continuous Delivery:

Authenticate to AWS

Push image to Amazon ECR

Update Kubernetes deployment image

Apply manifests

Wait for rollout completion

Rollout status is programmatically verified to prevent incomplete deployments.

ğŸ³ Docker Strategy

â€¢ Lightweight node:24-alpine base image
â€¢ Production-only dependencies (npm install --omit=dev)
â€¢ Optimized build layering for cache efficiency
â€¢ Image versioned using Git commit SHA
â€¢ Immutable container deployment

CrashLoopBackOff debugging revealed entrypoint path misalignment, which was resolved by aligning Docker CMD with the application start script.

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ app/                    # Node.js application
â”œâ”€â”€ infra/                  # Infrastructure-related configs (IAM, cluster files)
â”œâ”€â”€ k8s/                    # Kubernetes manifests
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ hpa.yaml
â”œâ”€â”€ .github/workflows/      # GitHub Actions CI/CD pipeline
â”œâ”€â”€ Dockerfile              # Production container definition
â””â”€â”€ README.md
ğŸ§ª Deployment Validation

During implementation, the following real-world issues were encountered and resolved:

â€¢ Docker build context misalignment
â€¢ Incorrect container entrypoint (MODULE_NOT_FOUND)
â€¢ CrashLoopBackOff debugging
â€¢ Kubernetes rollout timeout
â€¢ Invalid manifest validation errors
â€¢ IAM JSON mistakenly applied as Kubernetes resources

These were resolved through structured debugging using:

kubectl describe pod
kubectl logs
kubectl get events
kubectl rollout status
ğŸ§¹ Cost Management

In a production or cost-sensitive environment:

â€¢ EKS clusters would be destroyed using Infrastructure-as-Code
â€¢ Helm monitoring components would be removed
â€¢ ECR lifecycle policies would manage image retention

This ensures cloud cost control and operational hygiene.

ğŸ·ï¸ Release

v1.0.0 â€“ Production-Style DevSecOps Pipeline
